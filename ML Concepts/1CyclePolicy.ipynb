{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1CyclePolicy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gbqwrxbXcqV",
        "colab_type": "text"
      },
      "source": [
        "## 1-cycle learning\n",
        "\n",
        "This notebook covers the 1-cycle policy developed by Leslie Smith in which an optimal LR is chosen and we then train our network with a low LR and high momentum while increasing the LR to the optimal value whilst decreasing momentum. Then, we decrease LR again whilst increasing the momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O_wdv5uciUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import zipfile\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras as k\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruinPONUBPnB",
        "colab_type": "text"
      },
      "source": [
        "We define a callback to find the optimal learning rate as follows...\n",
        "\n",
        "We begin SGD with a very low LR (1e-7 here) which is adjusted (through multiplication) at each mini-batch until we reach a high LR. The loss is recorded for each LR and plotted at the end so we can visually decide on the optimal learning rate.\n",
        "\n",
        "However we don't just simply plot the loss, as this is erratic and would be difficult to discern an optimal value from. We instead plot a smoothed version of the loss by taking their exponentially weighted averages (much like in SGD with momentum) as such:\n",
        "\n",
        "> $avg\\_loss_i$ $=$ $\\beta$ $*$ $avg\\_loss_{i-1}$ $+$ $(1-\\beta)$ $*$ $loss_i$\n",
        "\n",
        "where $\\beta \\in (0,1)$ is the smoothing parameter. This gives\n",
        "\n",
        "> $avg\\_loss_i$ $=$ $\\beta$ $*$ $avg\\_loss_{i-1}$ $+$ $(1-\\beta)$ $*$ $loss_i$ \n",
        "\n",
        ">$=$ $\\beta^2$ $*$ $avg\\_loss_{i-2}$ $+$ $(1-\\beta)$ $*$ $loss_{i-1}$ $+$ $\\beta$ $*$ $loss_i$ \n",
        "\n",
        ">$...$\n",
        "\n",
        "> $=$ $(1-\\beta)\\beta^i loss_0 + (1-\\beta)\\beta^{i-1} loss_1 + ... + (1-\\beta)\\beta loss_{i-1} + (1-\\beta)loss_i$\n",
        "\n",
        "so our weights are all powers of $\\beta$, then from the sum of a geometric sequence we have our weight sum as:\n",
        "\n",
        "> $(1-\\beta)$ $*$ $\\frac{1-\\beta^{i+1}}{1-\\beta}=1-\\beta^{i+1}$\n",
        "\n",
        "We then divide our average loss by this value to obtain the smoothed loss, that is:\n",
        "\n",
        "> $smoothed\\_loss_i$ $=$ $\\frac{avg\\_loss_i}{1-\\beta^{i+1}}$\n",
        "\n",
        "We are interested in obtaining an ideal LR, so we should stop when the loss explodes, the following criteria is used in the FastAI library:\n",
        "\n",
        "> $current$ $smoothed\\_loss$ $> 4 \\times $ $minimum$ $smoothed\\_loss$\n",
        "\n",
        "Now to calculate the factor we multiply our learning rate by after each batch. Suppose our initial LR is $lr_0$ and the final LR is $lr_i$ and we multiply by a factor of $q$ each time. Then:\n",
        "\n",
        "> $lr_i = lr_0 \\times q^i$\n",
        "\n",
        "Now solving for $q$ gives:\n",
        "\n",
        "> $q^i=\\frac{lr_i}{lr_0} \\iff q = (\\frac{lr_i}{lr_0})^{\\frac{1}{i}}$\n",
        "\n",
        "With this we are ready to define the callback.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-ZcEy0vBQtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LRFinder(tf.keras.callbacks.Callback):\n",
        "\n",
        "  def __init__(self, start_lr = 1e-7, end_lr = 10, max_steps = 100, sf = 0.9):\n",
        "    super(LRFinder, self).__init__()\n",
        "    self.start_lr, self.end_lr = start_lr, end_lr\n",
        "    self.max_steps = max_steps\n",
        "    self.sf = sf\n",
        "    self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n",
        "    self.lrs, self.losses = [], []\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    # Zero values on train begin\n",
        "    self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n",
        "    self.lrs, self.losses = [], []\n",
        "\n",
        "  def on_train_batch_begin(self, batch, logs=None):\n",
        "    # After each batch update LR\n",
        "    self.lr = self.lr_anneal(self.step)\n",
        "    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):\n",
        "    logs = logs or {}\n",
        "    loss = logs.get('loss')\n",
        "    if loss:\n",
        "      # Calculate average loss as exponential weighted average\n",
        "      self.avg_loss = self.sf * self.avg_loss + (1 - self.sf) * loss\n",
        "      # Divide by sum of weights to obtain smoothed loss\n",
        "      smooth_loss = self.avg_loss / (1 - self.sf ** (self.step + 1))\n",
        "      self.losses.append(smooth_loss)\n",
        "      self.lrs.append(self.lr)\n",
        "\n",
        "      if self.step == 0 or loss < self.best_loss:\n",
        "        # Update if loss has decreased\n",
        "        self.best_loss = loss\n",
        "\n",
        "      if smooth_loss > 4 * self.best_loss or tf.math.is_nan(smooth_loss):\n",
        "        # Stop when loss explodes\n",
        "        self.model.stop_training = True\n",
        "\n",
        "    if self.step == self.max_steps:\n",
        "      # Stop when reached max_steps\n",
        "      self.model.stop_training = True\n",
        "\n",
        "    self.step += 1\n",
        "\n",
        "  def lr_anneal(self, step):\n",
        "    # Return next LR (multiply by the q in above explanation)\n",
        "    return self.start_lr * (self.end_lr / self.start_lr) ** (step * 1. / self.max_steps)\n",
        "\n",
        "  def plot(self, cutoff = None):\n",
        "    fig, ax = plt.subplots(1, 1)\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_xlabel('Learning Rate')\n",
        "    ax.set_xscale('log')\n",
        "    ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n",
        "    if cutoff:\n",
        "      ax.plot(self.lrs[:-cutoff], self.losses[:-cutoff])\n",
        "\n",
        "    else:\n",
        "      ax.plot(self.lrs, self.losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnpwd2XZMGG2",
        "colab_type": "text"
      },
      "source": [
        "With the above callback calculating our optimal LR, we now define a callback to implement the 1-cycle policy which is described as such:\n",
        "\n",
        "\n",
        "1.   Select a LR from the LR finder and a max and min momentum\n",
        "2.   Begin phase 1: Divide the LR by some factor and begin training with this value and the lower momentum. Gradually increase the LR to the chosen value whilst decreasing the momentum to the minimum using cosine annealing.\n",
        "3. Begin phase 2: Decrease the LR to approx. 0 whilst increasing momentum to maximum.\n",
        "\n",
        "This simple procedure leads to the phenomenon of super-convergence in which a neural net can be trained orders of magniture faster than usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXRe2ujKL3m6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CosineAnnealer:\n",
        "  # Anneal according to half a cosine period\n",
        "  def __init__(self, start, end, steps):\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.steps = steps\n",
        "    self.n = 0\n",
        "        \n",
        "  def step(self):\n",
        "    self.n += 1\n",
        "    cos = np.cos(np.pi * (self.n / self.steps)) + 1\n",
        "    return self.end + (self.start - self.end) / 2. * cos\n",
        "\n",
        "\n",
        "class OneCycleScheduler(Callback):\n",
        "\n",
        "  def __init__(self, lr_max, steps, mom_min=0.85, mom_max=0.95, phase_1_pct=0.3, div_factor=25.):\n",
        "    super(OneCycleScheduler, self).__init__()\n",
        "    # Phase 1 initial LR\n",
        "    lr_min = lr_max / div_factor\n",
        "    # Phase 2 final LR\n",
        "    final_lr = lr_max / (div_factor * 1e4)\n",
        "    # Phases do not need to be of equal steps\n",
        "    phase_1_steps = steps * phase_1_pct\n",
        "    phase_2_steps = steps - phase_1_steps\n",
        "        \n",
        "    self.phase_1_steps = phase_1_steps\n",
        "    self.phase_2_steps = phase_2_steps\n",
        "    self.phase = 0\n",
        "    self.step = 0\n",
        "        \n",
        "    # Get annealed values for LR and momentum using the CosineAnnealer class\n",
        "    self.phases = [[CosineAnnealer(lr_min, lr_max, phase_1_steps), CosineAnnealer(mom_max, mom_min, phase_1_steps)], \n",
        "                 [CosineAnnealer(lr_max, final_lr, phase_2_steps), CosineAnnealer(mom_min, mom_max, phase_2_steps)]]\n",
        "        \n",
        "    #self.lrs = []\n",
        "    #self.moms = []\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    self.phase = 0\n",
        "    self.step = 0\n",
        "    # Begin lr and momentum annealing schedules\n",
        "    self.set_lr(self.lr_schedule().start)\n",
        "    self.set_momentum(self.mom_schedule().start)\n",
        "        \n",
        "  #def on_train_batch_begin(self, batch, logs=None):\n",
        "    # store LR and mom for plotting\n",
        "    #self.lrs.append(self.get_lr())\n",
        "    #self.moms.append(self.get_momentum())\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):\n",
        "    self.step += 1\n",
        "    if self.step >= self.phase_1_steps:\n",
        "      # Switch to phase 2\n",
        "      self.phase = 1\n",
        "    # Update LR and mom\n",
        "    self.set_lr(self.lr_schedule().step())\n",
        "    self.set_momentum(self.mom_schedule().step())\n",
        "        \n",
        "  def get_lr(self):\n",
        "    try:\n",
        "      return tf.keras.backend.get_value(self.model.optimizer.lr)\n",
        "    except AttributeError:\n",
        "      return None\n",
        "        \n",
        "  def get_momentum(self):\n",
        "    try:\n",
        "      return tf.keras.backend.get_value(self.model.optimizer.momentum)\n",
        "    except AttributeError:\n",
        "      return None\n",
        "        \n",
        "  def set_lr(self, lr):\n",
        "    try:\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
        "    except AttributeError:\n",
        "        pass \n",
        "        \n",
        "  def set_momentum(self, mom):\n",
        "    try:\n",
        "      tf.keras.backend.set_value(self.model.optimizer.momentum, mom)\n",
        "    except AttributeError:\n",
        "      pass \n",
        "\n",
        "  def lr_schedule(self):\n",
        "    return self.phases[self.phase][0]\n",
        "    \n",
        "  def mom_schedule(self):\n",
        "    return self.phases[self.phase][1]\n",
        "    \n",
        "  #def plot(self):\n",
        "   # ax = plt.subplot(1, 2, 1)\n",
        "    #ax.plot(self.lrs)\n",
        "    #ax.set_title('Learning Rate')\n",
        "    #ax = plt.subplot(1, 2, 2)\n",
        "    #ax.plot(self.moms)\n",
        "    #ax.set_title('Momentum')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IEjexscR-uyd",
        "colab": {}
      },
      "source": [
        "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
        "\n",
        "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AQgFmELd-uXf",
        "colab": {}
      },
      "source": [
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MSXnAGq8-uKc",
        "colab": {}
      },
      "source": [
        "train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D8ENZUw6-tuu",
        "colab": {}
      },
      "source": [
        "num_cats_tr = len(os.listdir(train_cats_dir))\n",
        "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
        "\n",
        "num_cats_val = len(os.listdir(validation_cats_dir))\n",
        "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
        "\n",
        "total_train = num_cats_tr + num_dogs_tr\n",
        "total_val = num_cats_val + num_dogs_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oMUsB2MU-tL9",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 15\n",
        "IMG_HEIGHT = 150\n",
        "IMG_WIDTH = 150\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SWVr3IEK-suq",
        "colab": {}
      },
      "source": [
        "train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "da639601-9710-4f97-a29e-3442f84309f6",
        "id": "ZDUXDTgm-sPG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                           directory=train_dir,\n",
        "                                                           shuffle=True,\n",
        "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                           class_mode='binary')\n",
        "\n",
        "val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n",
        "                                                              directory=validation_dir,\n",
        "                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "                                                              class_mode='binary')\n"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d9Yfu_VV-sAY",
        "colab": {}
      },
      "source": [
        "def build_model(): \n",
        "  model = Sequential([\n",
        "      Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "      MaxPooling2D(),\n",
        "      Conv2D(32, 3, padding='same', activation='relu'),\n",
        "      MaxPooling2D(),\n",
        "      Conv2D(64, 3, padding='same', activation='relu'),\n",
        "      MaxPooling2D(),\n",
        "      Flatten(),\n",
        "      Dense(512, activation='relu'),\n",
        "      Dense(1)\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3_oaiKKRhUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04hJYkplRt3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_finder = LRFinder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX0FZVOoRsjs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "95ebc616-0352-4f3d-a988-b8403cef3092"
      },
      "source": [
        "_ = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=5,\n",
        "    callbacks=[lr_finder],\n",
        "    verbose=0\n",
        ")"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwYiDoGzSF1E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "2505c9d7-341b-4702-bd1d-da455dfb13c4"
      },
      "source": [
        "lr_finder.plot()"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw16yzgIUxW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1fOdGjqUxwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl4aE76wU5ix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "lr = 2e-3\n",
        "steps = np.ceil(total_train  / batch_size) * epochs\n",
        "lr_schedule = OneCycleScheduler(lr, steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4VW7ookU5k9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "d00496d5-9ecd-4033-ba2d-fe7c36bfe557"
      },
      "source": [
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size,\n",
        "    callbacks=[lr_schedule]\n",
        ")"
      ],
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train for 15 steps, validate for 7 steps\n",
            "Epoch 1/10\n",
            "15/15 [==============================] - 8s 507ms/step - loss: 0.7045 - accuracy: 0.5224 - val_loss: 0.7020 - val_accuracy: 0.4967\n",
            "Epoch 2/10\n",
            "15/15 [==============================] - 7s 492ms/step - loss: 0.6541 - accuracy: 0.5807 - val_loss: 0.6328 - val_accuracy: 0.6775\n",
            "Epoch 3/10\n",
            "15/15 [==============================] - 7s 497ms/step - loss: 0.5844 - accuracy: 0.6859 - val_loss: 0.6306 - val_accuracy: 0.6730\n",
            "Epoch 4/10\n",
            "15/15 [==============================] - 7s 491ms/step - loss: 0.5038 - accuracy: 0.7441 - val_loss: 0.5922 - val_accuracy: 0.6964\n",
            "Epoch 5/10\n",
            "15/15 [==============================] - 7s 495ms/step - loss: 0.4299 - accuracy: 0.7874 - val_loss: 0.6111 - val_accuracy: 0.6853\n",
            "Epoch 6/10\n",
            "15/15 [==============================] - 8s 503ms/step - loss: 0.3619 - accuracy: 0.8307 - val_loss: 0.6352 - val_accuracy: 0.6908\n",
            "Epoch 7/10\n",
            "15/15 [==============================] - 8s 502ms/step - loss: 0.2918 - accuracy: 0.8638 - val_loss: 0.7217 - val_accuracy: 0.6674\n",
            "Epoch 8/10\n",
            "15/15 [==============================] - 8s 507ms/step - loss: 0.2215 - accuracy: 0.9049 - val_loss: 0.6880 - val_accuracy: 0.7165\n",
            "Epoch 9/10\n",
            "15/15 [==============================] - 8s 505ms/step - loss: 0.1884 - accuracy: 0.9214 - val_loss: 0.6707 - val_accuracy: 0.7076\n",
            "Epoch 10/10\n",
            "15/15 [==============================] - 7s 488ms/step - loss: 0.1600 - accuracy: 0.9448 - val_loss: 0.6889 - val_accuracy: 0.7154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNQdiJveVQua",
        "colab_type": "text"
      },
      "source": [
        "We can now compare this to just running a model without the above procedures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bki0-C3hSOBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential([\n",
        "    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(1)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qEk3aQaP-ryJ",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f27cbddc-d729-4328-a7be-2f22966793be",
        "id": "sc4zPUfp-qbz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "history = model.fit(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=total_train // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=total_val // batch_size\n",
        ")\n"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train for 15 steps, validate for 7 steps\n",
            "Epoch 1/10\n",
            "15/15 [==============================] - 8s 506ms/step - loss: 1.0118 - accuracy: 0.5064 - val_loss: 0.6915 - val_accuracy: 0.4967\n",
            "Epoch 2/10\n",
            "15/15 [==============================] - 7s 486ms/step - loss: 0.6791 - accuracy: 0.5032 - val_loss: 0.6712 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "15/15 [==============================] - 7s 485ms/step - loss: 0.6338 - accuracy: 0.5951 - val_loss: 0.6299 - val_accuracy: 0.6462\n",
            "Epoch 4/10\n",
            "15/15 [==============================] - 7s 488ms/step - loss: 0.5817 - accuracy: 0.6757 - val_loss: 0.6191 - val_accuracy: 0.6317\n",
            "Epoch 5/10\n",
            "15/15 [==============================] - 7s 493ms/step - loss: 0.5371 - accuracy: 0.7137 - val_loss: 0.5945 - val_accuracy: 0.6897\n",
            "Epoch 6/10\n",
            "15/15 [==============================] - 7s 491ms/step - loss: 0.4771 - accuracy: 0.7532 - val_loss: 0.6115 - val_accuracy: 0.6719\n",
            "Epoch 7/10\n",
            "15/15 [==============================] - 7s 483ms/step - loss: 0.4288 - accuracy: 0.7927 - val_loss: 0.5966 - val_accuracy: 0.6786\n",
            "Epoch 8/10\n",
            "15/15 [==============================] - 7s 487ms/step - loss: 0.3908 - accuracy: 0.8034 - val_loss: 0.6156 - val_accuracy: 0.7054\n",
            "Epoch 9/10\n",
            "15/15 [==============================] - 7s 488ms/step - loss: 0.3349 - accuracy: 0.8371 - val_loss: 0.8422 - val_accuracy: 0.6674\n",
            "Epoch 10/10\n",
            "15/15 [==============================] - 7s 483ms/step - loss: 0.3647 - accuracy: 0.8243 - val_loss: 0.6420 - val_accuracy: 0.6819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcZ4TbGMXTFy",
        "colab_type": "text"
      },
      "source": [
        "We can see the 1-cycle policy does indeed allow for quicker convergence."
      ]
    }
  ]
}