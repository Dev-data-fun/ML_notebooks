{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k9WggrPYYJI",
        "colab_type": "text"
      },
      "source": [
        "# Variational Autoencoders on multiple datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUGZHU6AYe0h",
        "colab_type": "text"
      },
      "source": [
        "A convolutional VAE is used to generate images from some of the datasets in `tensorflow_datasets`. The model is able to handle RGB and gray-scale images. Some generation and reconstruction examples gifs are presented at the end of the notebook for a few datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Txzdif1bbe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q imageio # gif generation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slvQGqaabcEt",
        "colab_type": "code",
        "outputId": "8073d9d4-0f73-4eda-c3e7-4f589fd3d9f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import imageio\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXQxHkyu3Zsz",
        "colab_type": "text"
      },
      "source": [
        "An image dataset is loaded into training and testing sets. If no testing set exists for the dataset 500 random training samples are used in place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2d-HavmF-VE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET = 'horses_or_humans'\n",
        "\n",
        "train_dataset = tfds.load(name=DATASET, split=\"train\")\n",
        "\n",
        "try:\n",
        "  test_dataset = tfds.load(name=DATASET, split=\"test\")\n",
        "  print(\"Loaded test set.\")\n",
        "\n",
        "except AssertionError:\n",
        "  test_dataset = train_dataset.shuffle(1000).take(500)\n",
        "  print(\"No test set, generating from train.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraJeBOGwrvv",
        "colab_type": "text"
      },
      "source": [
        "Our images should be a square and preferably a power of 2 for easy convolutions. \n",
        "\n",
        "`CHANNELS = 3` for RGB, `CHANNELS = 1` for gray-scale.\n",
        "\n",
        "`preprocess` resizes the images and normalizes between [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysCwwhNjaTIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_X, IMG_Y = 128, 128  # desired X, Y size for images\n",
        "\n",
        "CHANNELS = 3\n",
        "\n",
        "def preprocess(element):\n",
        "  # Resize and normalize\n",
        "  element['image'] = tf.image.resize(element['image'], size=(IMG_X,IMG_Y))  \n",
        "  element['image'] = tf.cast(element['image'], dtype = tf.float32) / 255.\n",
        "  \n",
        "  return element"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmGpYY_DGVjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = train_dataset.map(preprocess)\n",
        "test_dataset = test_dataset.map(preprocess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7pD8lREjhdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = train_dataset.shuffle(1000).batch(100)\n",
        "test_dataset = test_dataset.batch(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG_6sJQFGRcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_images(fig, init_str, end_str):\n",
        "  # saves a pyplot figure\n",
        "  plt.savefig('{}_at_epoch_{:04d}.png'.format(init_str, end_str))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjaQmCFPsk1U",
        "colab_type": "text"
      },
      "source": [
        "### Encoder model\n",
        "\n",
        "The encoder receives the input and performs sequential convolutions to reduce the signal size. A kernel size of 3 and stride of 2 ensures we halve the signal size. That is, a 64 x 64 image will be reduced to 32 x 32.\n",
        "\n",
        "The extracted features are flattened and passed to a standard fully connected layer, this is a low dimensional representation of the input data (VAEs encode each input as a distribution over this latent space - this ensures the latent space is regular, as opposed to traditional autoencoders). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n86X4l-7s6qn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class encoder(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self, X, Y, Channels):\n",
        "    super(encoder, self).__init__()\n",
        "\n",
        "    self.x = X\n",
        "    self.y = Y\n",
        "    self.channels = Channels\n",
        "\n",
        "    if self.channels==1: \n",
        "      self.inputLayer = layers.InputLayer(input_shape=(self.x, self.y))\n",
        "    else:\n",
        "      self.inputLayer = layers.InputLayer(input_shape=(self.x, self.y, self.channels))\n",
        "\n",
        "    self.conv1 = layers.Conv2D(\n",
        "              filters=32, kernel_size=3, strides=(2,2), activation='relu')\n",
        "    self.conv2 = layers.Conv2D(\n",
        "              filters=64, kernel_size=3, strides=(2,2), activation='relu')\n",
        "    self.flatten = layers.Flatten()\n",
        "    self.dense_in = layers.Dense(latent_dim + latent_dim)\n",
        "\n",
        "  def call(self, x): # forward pass\n",
        "    x = self.conv1(self.inputLayer(x))\n",
        "    x = self.conv2(x)\n",
        "    x = self.dense_in(self.flatten(x))\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqiJbpNTs7Go",
        "colab_type": "text"
      },
      "source": [
        "### Decoder Model\n",
        "\n",
        "The decoder receives a sampled low dimensional representation from the latent space and attempts to reproduce the original input through sequential deconvolutions. The output will have the same shape as the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3hN_hMbbmxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class decoder(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self, X, Y, Channels):\n",
        "    super(decoder, self).__init__()\n",
        "\n",
        "    self.x = X\n",
        "    self.y = Y\n",
        "    self.channels = Channels\n",
        "\n",
        "    self.inputLayer = layers.InputLayer(input_shape=(latent_dim,))\n",
        "\n",
        "    # Image has been halved twice by the encoder\n",
        "    self.dense_out = layers.Dense(units=(self.x//4)*(self.y//4)*32,                           \n",
        "                                  activation=tf.nn.relu)\n",
        "    \n",
        "    self.reshape = layers.Reshape(target_shape=(self.x//4, self.y//4, 32))\n",
        "\n",
        "    self.convT1 = layers.Conv2DTranspose(filters=64, kernel_size=3, \n",
        "                                         strides=(2,2), padding=\"SAME\", \n",
        "                                         activation='relu')\n",
        "    \n",
        "    self.convT2 = layers.Conv2DTranspose(filters=32, kernel_size=3,\n",
        "                                         strides=(2,2), padding=\"SAME\", \n",
        "                                         activation='relu')\n",
        "    \n",
        "    # Output same number of input channels (3-RGB 1-Gray)\n",
        "    self.convT3 = layers.Conv2DTranspose(filters=self.channels, kernel_size=3, \n",
        "                                         strides=(1, 1), padding=\"SAME\")\n",
        "\n",
        "  def call(self, x): # forward pass\n",
        "    x = self.dense_out(self.inputLayer(x))\n",
        "    x = self.convT1(self.reshape(x))\n",
        "    x = self.convT2(x)\n",
        "    x = self.convT3(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLoi7rlvs-A1",
        "colab_type": "text"
      },
      "source": [
        "### VAE\n",
        "\n",
        "The VAE is simply the encoder decoder pair. An input is passed to the VAE and encoded as a distribution over the latent space. We sample a point from this distribution and decode it, allowing reconstruction error to be computed.\n",
        "\n",
        "The network is regularised by enforcing distributions to be close to a [standard normal distribution](https://stattrek.com/statistics/dictionary.aspx?definition=standard_normal_distribution), resulting in a continuous latent space. This space can be visualised nicely by setting the latent dimension to a size of 2.\n",
        "\n",
        "To save a visualization of the input and reconstructed outputs, set `save_reconstruction` to `True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSFnmolds_Xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CVAE(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, X, Y, Channels, latent_dim, save_reconstruction):\n",
        "    super(CVAE, self).__init__()\n",
        "\n",
        "    self.latent_dim = latent_dim  # Size of latent space, \n",
        "    self.x = X  # X size of image\n",
        "    self.y = Y  # Y size of image\n",
        "    self.channels = Channels  # Image channels\n",
        "    self.save = save_reconstruction # Save reconstruction of inputs\n",
        "\n",
        "    self.encoder = encoder(self.x, self.y, self.channels)\n",
        "    self.decoder = decoder(self.x, self.y, self.channels)\n",
        "\n",
        "  @tf.function\n",
        "  def sample(self, eps=None):\n",
        "    # Generate random vector from latent space and decode \n",
        "    # for generating new images\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "    return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "  def encode(self, x):\n",
        "    # Mean and logvariance of the encoded input are taken for sampling\n",
        "    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "    return mean, logvar\n",
        "\n",
        "  def reparameterize(self, mean, logvar):\n",
        "    # Allows for gradient descent: sample from Gauss., multiply by std, add mean\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "  def decode(self, z, apply_sigmoid=False):\n",
        "    logits = self.decoder(z)\n",
        "    if apply_sigmoid:\n",
        "      probs = tf.sigmoid(logits)\n",
        "      return probs\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w6z7pK6kd8c",
        "colab_type": "text"
      },
      "source": [
        "### [Kullback–Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (KL divergence)\n",
        "\n",
        " Let `X` be our data, `P(X)` be its probability distribution, `P(z)` be the probability distribution of the latent variable. `P(z|X)` is the distribution which projects our data into the latent space, this is estimated by a distribution `Q`. During training, the VAE aims to learn a `Q(z|X)` that is as close to the actual distribution `P(z|X)`. The KL divergence is the measure of difference between these distributions.\n",
        "\n",
        "We now define the loss function and gradient propagation, code taken from [here](https://www.tensorflow.org/tutorials/generative/cvae)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIuv4x0BbqRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "  log2pi = tf.math.log(2. * np.pi)\n",
        "  return tf.reduce_sum(\n",
        "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "      axis=raxis)\n",
        "\n",
        "@tf.function\n",
        "def compute_loss(model, x):\n",
        "  mean, logvar = model.encode(x)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  x_logit = model.decode(z)\n",
        "\n",
        "  # Calculate distributions\n",
        "\n",
        "  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "  logpz = log_normal_pdf(z, 0., 0.)\n",
        "  logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "\n",
        "  # Maximizing ELBO <==> Minimizing KL\n",
        "\n",
        "  return -tf.reduce_mean(logpx_z + logpz - logqz_x) # Monte Carlo estimate of ^\n",
        "\n",
        "@tf.function\n",
        "def compute_apply_gradients(model, x, optimizer):\n",
        "\n",
        "  # Prop gradients, optimize\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = compute_loss(model, x)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84hdTbpKpLS_",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to train the model.\n",
        "\n",
        "`latent_dim` is the size of the latent space, \n",
        "`num_examples` is the number of new images to generate (edit axis in `generate_images` if you change from 16).\n",
        "\n",
        "`generate_images` passes the random vector `gen_vect` to the model to generate new images. These are plotted and saved for animation later.\n",
        "\n",
        "`reconstruct_save` plots and saves input and reconstructed outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvJMX-TPbrd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 200  \n",
        "latent_dim = 100\n",
        "num_examples = 16\n",
        "\n",
        "# Random vect for generation visualization\n",
        "\n",
        "gen_vect = tf.random.normal(shape=[num_examples, latent_dim]) \n",
        "\n",
        "model = CVAE(IMG_X, IMG_Y, CHANNELS, latent_dim, True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2foblL9buik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_images(model, epoch, test_input):\n",
        "  \n",
        "  # Generates new images with model, plots and saves them\n",
        "\n",
        "  preds = model.sample(test_input)\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "\n",
        "  for i in range(preds.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      if CHANNELS==1:\n",
        "        image = preds[i, :, :, 0]\n",
        "        plt.imshow(image, cmap='gray')\n",
        "      else:\n",
        "        image = preds[i, :, :, :]\n",
        "        plt.imshow(image, cmap=plt.cm.binary)\n",
        "      plt.axis('off')\n",
        "\n",
        "  save_images(fig, 'gen', epoch)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt27ITcmEqVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reconstruct_save(dataset, samples, epoch):\n",
        "\n",
        "  # Save a set of input images and their reconstruction\n",
        "\n",
        "  global model\n",
        "\n",
        "  x = dataset\n",
        "  mean, logvar = model.encode(x)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  preds = model.decode(z, apply_sigmoid=True)\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  idx = 0\n",
        "\n",
        "  for i in range(0, x.shape[0]*2, 2): # To plot input and reconstruction side by side\n",
        "    plt.subplot(samples, 2, i+1)\n",
        "    if CHANNELS == 1:\n",
        "      plt.imshow(tf.squeeze(x[idx]), cmap='gray')\n",
        "      plt.axis('off')\n",
        "      plt.subplot(samples, 2, i+2)\n",
        "      plt.imshow(tf.squeeze(preds[idx]), cmap='gray')\n",
        "    else:\n",
        "      plt.imshow(x[idx])\n",
        "      plt.axis('off')\n",
        "      plt.subplot(samples, 2, i+2)\n",
        "      plt.imshow(preds[idx])\n",
        "\n",
        "    idx+=1\n",
        "    plt.axis('off')\n",
        "\n",
        "  save_images(fig, 'reconstructed', epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkAayeu8NCAh",
        "colab_type": "text"
      },
      "source": [
        "We now generate our own images as the model trains. \n",
        "\n",
        "`samples_to_save` specifies how many reconstruction samples to save."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfx29XRkbvrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples_to_save = 10\n",
        "\n",
        "generate_images(model, 0, gen_vect)\n",
        "\n",
        "if model.save:\n",
        "  for batch in test_dataset.take(1):\n",
        "    # To visualize reconstruction as training progresses\n",
        "    visual_set = batch['image'][:samples_to_save] \n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  start_time = time.time()\n",
        "  for train_x in train_dataset:\n",
        "    # train\n",
        "    compute_apply_gradients(model, train_x['image'], optimizer)\n",
        "  end_time = time.time()\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    loss = tf.keras.metrics.Mean()\n",
        "    for test_x in test_dataset:\n",
        "      # test\n",
        "      loss(compute_loss(model, test_x['image']))\n",
        "    elbo = -loss.result() \n",
        "    display.clear_output(wait=False)\n",
        "    print('Epoch: {}, Test set ELBO: {}, '\n",
        "          'time elapse for current epoch {}'.format(epoch, elbo,\n",
        "                                                    end_time - start_time))\n",
        "    generate_images(model, epoch, gen_vect)\n",
        "\n",
        "  if model.save:\n",
        "    reconstruct_save(visual_set, samples_to_save, epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEZQwsPFYKo9",
        "colab_type": "text"
      },
      "source": [
        "We can create a gif of our saved images for better visualization, `save_gif` saves a gif with name `name` formed from images beginning with `lead_str` (`lead_str` = `'reconstructed'` for the reconstructed images, `'gen'` for the generated images)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0qttv-FS0YH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def save_gif(name, lead_str): \n",
        "\n",
        "   # Saves gif from sequence of images\n",
        "\n",
        "  anim_file = name+'.gif'\n",
        "\n",
        "  with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "    filenames = glob.glob(lead_str+'*.png')\n",
        "    filenames = sorted(filenames)\n",
        "    last = -1\n",
        "    for i,filename in enumerate(filenames):\n",
        "      frame = 2*(i**0.5)\n",
        "      if round(frame) > round(last):\n",
        "        last = frame\n",
        "      else:\n",
        "        continue\n",
        "      image = imageio.imread(filename)\n",
        "      writer.append_data(image)\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "\n",
        "  import IPython\n",
        "  if IPython.version_info >= (6,2,0,''):\n",
        "    display.Image(filename=anim_file)\n",
        "\n",
        "  files.download(anim_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saW1TfTqsyk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_gif('horseGen', 'gen')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmPUGb1btFyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_gif('horseRecon', 'reconstructed')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo8gIrVItQvz",
        "colab_type": "text"
      },
      "source": [
        "### Some generations and reconstructions on different datasets\n",
        "\n",
        "Datasets were trained for 50-200 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swuTcoUptzP0",
        "colab_type": "text"
      },
      "source": [
        "Reconstruction of flowers from 'tf_flowers' dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU0dzrLEtYga",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://i.imgur.com/HYSNRhL.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ojRhC2Zu7je",
        "colab_type": "text"
      },
      "source": [
        "Generation and reconstruction of Kuzushiji figures from the 'kmnist' dataset:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T98AYNmSw79Q",
        "colab_type": "text"
      },
      "source": [
        "![kmnist_gen](https://imgur.com/81OzBLU.gif)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7JjfMKvxG-A",
        "colab_type": "text"
      },
      "source": [
        "![kmnist_recon](https://i.imgur.com/GkT9kBi.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1KgmgDcxaqF",
        "colab_type": "text"
      },
      "source": [
        "Generation and reconstruction of horses and humans from 'horses_or_humans' dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJVBaaMfxiXt",
        "colab_type": "text"
      },
      "source": [
        "![horse_gen](https://imgur.com/vKs2w7D.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZCcWIB7xiau",
        "colab_type": "text"
      },
      "source": [
        "![horse_recon](https://imgur.com/4j0dgjt.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfcNTUhDtHUV",
        "colab_type": "text"
      },
      "source": [
        "We may even download our own images and train the model on those. Using a script 200 images of Donald Trump were downloaded from Google Images. The model generated the following image after some training, an orange face in a suit:\n",
        "\n",
        "![trump](https://imgur.com/Wh1jKMx.png)"
      ]
    }
  ]
}